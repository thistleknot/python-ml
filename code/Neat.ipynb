{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44696398-216f-4dcc-bac5-ddc007c83908",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a710ab1f-055c-49a4-8637-11ba5401c224",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1649322-6ff6-4f8f-8022-f48b0a9e84e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408\n",
      "Discrete(6)\n",
      "Timesteps taken: 2209\n",
      "Penalties incurred: 743\n"
     ]
    }
   ],
   "source": [
    "observation = env.reset()\n",
    "print(observation)\n",
    "print(env.action_space)\n",
    "\n",
    "env.s = 328  # set environment to illustration's state\n",
    "\n",
    "epochs = 0\n",
    "penalties, reward = 0, 0\n",
    "\n",
    "frames = [] # for animation\n",
    "\n",
    "done = False\n",
    " \n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, done, info = env.step(action)\n",
    "    #print(state)\n",
    "\n",
    "    if reward == -10:\n",
    "        penalties += 1\n",
    "    \n",
    "    # Put each rendered frame into dict for animation\n",
    "    frames.append({\n",
    "        'frame': env.render(mode='ansi'),\n",
    "        'state': state,\n",
    "        'action': action,\n",
    "        'reward': reward\n",
    "        }\n",
    "    )\n",
    "\n",
    "    epochs += 1\n",
    "    \n",
    "    \n",
    "print(\"Timesteps taken: {}\".format(epochs))\n",
    "print(\"Penalties incurred: {}\".format(penalties))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd89213-6e23-4351-b6b6-3ac42a857973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ee7471-8f29-4129-ba65-3dd7a74dfeb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ****** Running generation 0 ****** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import gym\n",
    "import numpy as np\n",
    "import neat\n",
    "\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "       \n",
    "observation = env.reset()\n",
    "\n",
    "def eval_genomes(genomes, config):\n",
    "    for genome_id, genome in genomes:\n",
    "        genome.fitness = eval_genome(genome, config)\n",
    "\n",
    "local_dir = os.path.dirname(os.getcwd())\n",
    "config_path = os.path.join(local_dir, 'code\\config-feedforward.cfg')\n",
    "config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "                     neat.DefaultSpeciesSet, neat.DefaultStagnation,\n",
    "                     config_path)\n",
    "\n",
    "def eval_genome(genome, config):\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "    print(net.activate(observation))\n",
    "    \n",
    "pop = neat.Population(config)\n",
    "stats = neat.StatisticsReporter()\n",
    "pop.add_reporter(stats)\n",
    "pop.add_reporter(neat.StdOutReporter(True))\n",
    "\n",
    "pe = neat.ParallelEvaluator(1, eval_genome)\n",
    "winner = pop.run(pe.evaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f799960-dcff-4819-bb58-33bd2b5d411c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2b5b3d-b808-40f4-867a-06af0823bf20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ****** Running generation 0 ****** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from __future__ import print_function\n",
    "\n",
    "import multiprocessing\n",
    "import os\n",
    "import pickle\n",
    "import gym\n",
    "import numpy as np\n",
    "import neat\n",
    "import random\n",
    "\n",
    "#import cart_pole\n",
    "#import visualize\n",
    "\n",
    "runs_per_net = 2 \n",
    "simulation_seconds = 60.0\n",
    "\n",
    "#env = gym.make(\"Taxi-v3\").env\n",
    "# reset environment to a new, random state\n",
    "\n",
    "total_rewards, total_epochs, total_penalties = 0, 0, 0\n",
    "episodes = 100\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.01\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "# Use the NN network phenotype and the discrete actuator force function.\n",
    "def eval_genome(genome, config):\n",
    "    net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "\n",
    "    fitnesses = []\n",
    "\n",
    "    for runs in range(runs_per_net):\n",
    "        env = gym.make(\"Taxi-v3\").env\n",
    "        \n",
    "        q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "        #sim = cart_pole.CartPole()\n",
    "        state = env.reset()\n",
    "        #observation = env.reset()\n",
    "\n",
    "        # Run the given simulation for up to num_steps time steps.\n",
    "\n",
    "        rewards = 0\n",
    "        epochs = 0\n",
    "        penalties, reward = 0, 0\n",
    "        fitness = 0.0\n",
    "        '''\n",
    "        while sim.t < simulation_seconds:\n",
    "            inputs = sim.get_scaled_state()\n",
    "            action = net.activate(inputs)\n",
    "\n",
    "            # Apply action to the simulated cart-pole\n",
    "            force = cart_pole.discrete_actuator_force(action)\n",
    "            sim.step(force)\n",
    "\n",
    "            # Stop if the network fails to keep the cart within the position or angle limits.\n",
    "            # The per-run fitness is the number of time steps the network can balance the pole\n",
    "            # without exceeding these limits.\n",
    "            if abs(sim.x) >= sim.position_limit or abs(sim.theta) >= sim.angle_limit_radians:\n",
    "                break\n",
    "\n",
    "            fitness = sim.t\n",
    "        '''\n",
    "        done = False\n",
    "\n",
    "        #frames = []        \n",
    "        \n",
    "        while not done:\n",
    "            if random.uniform(0, 1) < epsilon:\n",
    "                action = env.action_space.sample() # Explore action space\n",
    "            else:\n",
    "                #action = np.argmax(q_table[state]) # Exploit learned values\n",
    "                action = np.argmax(net.activate(q_table[state]))\n",
    "            \n",
    "            next_state, reward, done, info = env.step(action) \n",
    "            \n",
    "            old_value = q_table[state, action]\n",
    "            next_max = np.max(q_table[next_state])\n",
    "\n",
    "            new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "            q_table[state, action] = new_value\n",
    "            #print(observation)\n",
    "\n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "\n",
    "            # Put each rendered frame into dict for animation\n",
    "            '''\n",
    "            frames.append({\n",
    "                'frame': env.render(mode='ansi'),\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'reward': reward\n",
    "                }\n",
    "            )\n",
    "            '''\n",
    "\n",
    "            rewards += reward\n",
    "            epochs += 1   \n",
    "            print(penalties/epochs)\n",
    "            \n",
    "        fitness = penalties/epochs\n",
    "        print(fitness)\n",
    "    \n",
    "        #total_rewards +- rewards\n",
    "        #total_penalties += penalties\n",
    "        #total_epochs += epochs            \n",
    "        \n",
    "        #print(\"Action Space {}\".format(env.action_space))\n",
    "        #print(\"Observation Space {}\".format(env.observation_space))   \n",
    "        #print(f\"Average rewards per episode: {total_rewards / runs_per_net}\")\n",
    "        #print(f\"Average timesteps per episode: {total_epochs / runs_per_net}\")\n",
    "        #print(f\"Average penalties per episode: {total_penalties / runs_per_net}\")        \n",
    "\n",
    "        fitnesses.append(fitness)\n",
    "\n",
    "    # The genome's fitness is its worst performance across all runs.\n",
    "    return np.mean(fitnesses)\n",
    "\n",
    "\n",
    "def eval_genomes(genomes, config):\n",
    "    for genome_id, genome in genomes:\n",
    "        genome.fitness = eval_genome(genome, config)\n",
    "\n",
    "\n",
    "def run():\n",
    "    # Load the config file, which is assumed to live in\n",
    "    # the same directory as this script.\n",
    "    local_dir = os.path.dirname(os.getcwd())\n",
    "    config_path = os.path.join(local_dir, 'code\\config-feedforward.cfg')\n",
    "    config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "                         neat.DefaultSpeciesSet, neat.DefaultStagnation,\n",
    "                         config_path)\n",
    "\n",
    "    pop = neat.Population(config)\n",
    "    stats = neat.StatisticsReporter()\n",
    "    pop.add_reporter(stats)\n",
    "    pop.add_reporter(neat.StdOutReporter(True))\n",
    "\n",
    "    pe = neat.ParallelEvaluator(multiprocessing.cpu_count(), eval_genome)\n",
    "    winner = pop.run(pe.evaluate)\n",
    "\n",
    "    # Save the winner.\n",
    "    with open('winner-feedforward', 'wb') as f:\n",
    "        pickle.dump(winner, f)\n",
    "\n",
    "    print(winner)\n",
    "\n",
    "    visualize.plot_stats(stats, ylog=True, view=True, filename=\"feedforward-fitness.svg\")\n",
    "    visualize.plot_species(stats, view=True, filename=\"feedforward-speciation.svg\")\n",
    "\n",
    "    node_names = {-1: 'x', -2: 'dx', -3: 'theta', -4: 'dtheta', 0: 'control'}\n",
    "    visualize.draw_net(config, winner, True, node_names=node_names)\n",
    "\n",
    "    visualize.draw_net(config, winner, view=True, node_names=node_names,\n",
    "                       filename=\"winner-feedforward.gv\")\n",
    "    visualize.draw_net(config, winner, view=True, node_names=node_names,\n",
    "                       filename=\"winner-feedforward-enabled.gv\", show_disabled=False)\n",
    "    visualize.draw_net(config, winner, view=True, node_names=node_names,\n",
    "                       filename=\"winner-feedforward-enabled-pruned.gv\", show_disabled=False, prune_unused=True)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4163e2f-c0bb-499f-9e47-d88b58332140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\user\\\\Documents\\\\wiki\\\\wiki\\\\dev\\\\python\\\\python-ml\\\\code'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
