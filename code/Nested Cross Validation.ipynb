{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92de849c-fc6a-4fb9-ac6f-186d17b2b923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# automatic nested cross-validation of regression of 'Linear Regression using Step Forward Feature Selection' contrasted with 'Elastic Net' using pipelines as well as ZCA whitening to scale variables..\n",
    "\n",
    "#https://machinelearningmastery.com/nested-cross-validation-for-machine-learning-with-python/\n",
    "#https://machinelearningmastery.com/elastic-net-regression-in-python/\n",
    "#https://github.com/rasbt/mlxtend/issues/41\n",
    "#https://github.com/rasbt/mlxtend/issues/69\n",
    "#https://stackoverflow.com/questions/53252156/standardscaler-with-pipelines-and-gridsearchcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbb1adca-d35a-4671-9667-bffad54974e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "from numpy import arange\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from random import sample\n",
    "\n",
    "import random\n",
    "\n",
    "import re\n",
    "\n",
    "#from sklearn.datasets import make_classification\n",
    "from sklearn import linear_model\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import sys\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    import warnings\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc685d3a-564a-4092-a219-9eef11441886",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# ------------------------------------\n",
    "# file: zca.py\n",
    "# date: Thu May 21 15:47 2015\n",
    "# author:\n",
    "# Maarten Versteegh\n",
    "# github.com/mwv\n",
    "# maartenversteegh AT gmail DOT com\n",
    "#\n",
    "# Licensed under GPLv3\n",
    "# ------------------------------------\n",
    "\"\"\"zca: ZCA whitening with a sklearn-like interface\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.utils.validation import check_is_fitted\n",
    "from sklearn.utils import check_array, as_float_array\n",
    "\n",
    "class ZCA(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, regularization=1e-6, copy=False):\n",
    "        self.regularization = regularization\n",
    "        self.copy = copy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Compute the mean, whitening and dewhitening matrices.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like with shape [n_samples, n_features]\n",
    "            The data used to compute the mean, whitening and dewhitening\n",
    "            matrices.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse=None, copy=self.copy,\n",
    "                        ensure_2d=True)\n",
    "        X = as_float_array(X, copy=self.copy)\n",
    "        self.mean_ = X.mean(axis=0)\n",
    "        X_ = X - self.mean_\n",
    "        cov = np.dot(X_.T, X_) / (X_.shape[0]-1)\n",
    "        U, S, _ = linalg.svd(cov)\n",
    "        s = np.sqrt(S.clip(self.regularization))\n",
    "        s_inv = np.diag(1./s)\n",
    "        s = np.diag(s)\n",
    "        self.whiten_ = np.dot(np.dot(U, s_inv), U.T)\n",
    "        self.dewhiten_ = np.dot(np.dot(U, s), U.T)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None, copy=None):\n",
    "        \"\"\"Perform ZCA whitening\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like with shape [n_samples, n_features]\n",
    "            The data to whiten along the features axis.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'mean_')\n",
    "        X = as_float_array(X, copy=self.copy)\n",
    "        return np.dot(X - self.mean_, self.whiten_.T)\n",
    "\n",
    "    def inverse_transform(self, X, copy=None):\n",
    "        \"\"\"Undo the ZCA transform and rotate back to the original\n",
    "        representation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like with shape [n_samples, n_features]\n",
    "            The data to rotate back.\n",
    "        \"\"\"\n",
    "        check_is_fitted(self, 'mean_')\n",
    "        X = as_float_array(X, copy=self.copy)\n",
    "        return np.dot(X, self.dewhiten_) + self.mean_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53dae0be-aa5b-41a6-ac79-03045466367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    df = pd.read_csv(\"https://raw.githubusercontent.com/thistleknot/Python-Stock/master/data/raw/states.csv\").set_index('States')\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a98ab490-94f1-49b3-97f0-d4d0e3e331c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "independent = 'Poverty'\n",
    "outer_k = 10\n",
    "inner_k = 10\n",
    "random_st = random.sample(list(np.arange(0,10,1)),1)[0]\n",
    "print(random_st)\n",
    "\n",
    "# configure the cross-validation procedure\n",
    "cv_inner = KFold(n_splits=inner_k, shuffle=True, random_state=random_st)\n",
    "cv_outer = KFold(n_splits=outer_k, shuffle=True, random_state=random_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f0f0f45-3357-492d-86de-7aabd366e6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset\n",
    "y = read_data()[[independent]]\n",
    "X = read_data()[(read_data().columns).difference([independent]).values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a486e8c-f677-4363-9c23-b5af018984de",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbd4a0d6-a64f-42a7-9d96-951381e3bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8ab64fb-89be-40c3-9ca9-cd3665261f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Crime', 'Doctors', 'Income', 'Population', 'Unemployed', 'White'], dtype='object')\n",
      "Index(['Crime', 'Doctors', 'Income', 'Infant Mort', 'Population',\n",
      "       'Traf Deaths', 'Unemployed', 'University', 'White'],\n",
      "      dtype='object')\n",
      "Index(['Crime', 'Doctors', 'Income', 'Infant Mort', 'Population',\n",
      "       'Traf Deaths', 'Unemployed', 'University', 'White'],\n",
      "      dtype='object')\n",
      "Index(['Crime', 'Doctors', 'Income', 'Infant Mort', 'Population',\n",
      "       'Traf Deaths', 'Unemployed', 'University', 'White'],\n",
      "      dtype='object')\n",
      "Index(['Crime', 'Doctors', 'Income', 'Infant Mort', 'Population',\n",
      "       'Traf Deaths', 'Unemployed', 'University', 'White'],\n",
      "      dtype='object')\n",
      "Index(['Crime', 'Doctors', 'Income', 'Population', 'White'], dtype='object')\n",
      "Index(['Crime', 'Doctors', 'Income', 'Infant Mort', 'Population',\n",
      "       'Traf Deaths', 'Unemployed', 'University', 'White'],\n",
      "      dtype='object')\n",
      "Index(['Crime', 'Doctors', 'Income', 'Infant Mort', 'Population',\n",
      "       'Traf Deaths', 'Unemployed', 'University', 'White'],\n",
      "      dtype='object')\n",
      "Index(['Crime', 'Doctors', 'Income', 'Infant Mort', 'Population',\n",
      "       'Traf Deaths', 'Unemployed', 'University', 'White'],\n",
      "      dtype='object')\n",
      "Index(['Crime', 'Doctors', 'Income', 'Infant Mort', 'Population',\n",
      "       'Traf Deaths', 'Unemployed', 'University', 'White'],\n",
      "      dtype='object')\n",
      "accuracy: 2.518 (2.477)\n"
     ]
    }
   ],
   "source": [
    "#elastic\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1,shuffle=True)\n",
    "ratios = arange(0, 1, 0.1)\n",
    "alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]\n",
    "model = ElasticNetCV(l1_ratio=ratios, alphas=alphas, cv=cv, n_jobs=-1)\n",
    "\n",
    "outer_results = list()\n",
    "\n",
    "# perform cross-validation procedure\n",
    "for train_ix, test_ix in cv_outer.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "\n",
    "    \n",
    "    pipe_ENet_Regressor = Pipeline([\n",
    "                #('scaler',  ZCA()),\n",
    "                ('ENet_Regressor', model)])\n",
    "\n",
    "    grid_params_ENet_Regressor = [{\n",
    "    }]\n",
    "\n",
    "    CV_ENet_regressor = GridSearchCV (estimator = pipe_ENet_Regressor,\n",
    "                                   param_grid = grid_params_ENet_Regressor,\n",
    "                                   #cv = cv,return_train_score=True, verbose=0, refit=True)\n",
    "                                   return_train_score=True, verbose=0, refit=True)\n",
    "\n",
    "    CV_ENet_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    best_model = CV_ENet_regressor.best_estimator_\n",
    "    #ypred=CV_ENet_regressor.predict(X_test)\n",
    "    yhat = best_model.predict(X_test)\n",
    "    \n",
    "    print(X.columns[best_model.named_steps['ENet_Regressor'].coef_!=0])\n",
    "    \n",
    "    # evaluate the model\n",
    "    acc = mean_squared_error(y_test, yhat,squared=True)\n",
    "    #cross_val_score(self.search, X, y, scoring='neg_mean_squared_error', cv=cv_outer, n_jobs=-1)\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    \n",
    "print('accuracy: %.3f (%.3f)' % (mean(outer_results), std(outer_results)))\n",
    "\n",
    "model_results.append(['Elastic',mean(outer_results), std(outer_results)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83edbfa2-7d8d-4032-bb17-15b6e5d85807",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2d6c96f-ab3d-4914-baf4-5590098c7628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Income', 'Population', 'Traf Deaths', 'Unemployed', 'White'], dtype='object')\n",
      "Index(['Income', 'Population', 'Traf Deaths', 'Unemployed', 'White'], dtype='object')\n",
      "Index(['Income', 'Population', 'Traf Deaths', 'Unemployed', 'White'], dtype='object')\n",
      "Index(['Income', 'Traf Deaths', 'Unemployed', 'White'], dtype='object')\n",
      "Index(['Income', 'Population', 'Traf Deaths', 'Unemployed', 'White'], dtype='object')\n",
      "Index(['Crime', 'Doctors', 'Income', 'Population', 'White'], dtype='object')\n",
      "Index(['Income', 'Population', 'Traf Deaths', 'Unemployed', 'White'], dtype='object')\n",
      "Index(['Doctors', 'Income', 'Infant Mort', 'Unemployed', 'White'], dtype='object')\n",
      "Index(['Income', 'Population', 'Traf Deaths', 'Unemployed', 'White'], dtype='object')\n",
      "Index(['Income', 'Population', 'Traf Deaths', 'Unemployed', 'White'], dtype='object')\n",
      "accuracy: 2.391 (2.143)\n"
     ]
    }
   ],
   "source": [
    "#linear\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1,shuffle=True)\n",
    "#model = ElasticNetCV(l1_ratio=0, alphas=[0], n_jobs=-1)\n",
    "model = LinearRegression()\n",
    "\n",
    "outer_results = list()\n",
    "\n",
    "class sfs_(SFS):        \n",
    "    def score(self):\n",
    "        return np.min(np.abs([self.subsets_[i]['avg_score'] for i in range(1,len(self.subsets_))]))*-1\n",
    "\n",
    "# perform cross-validation procedure\n",
    "for train_ix, test_ix in cv_outer.split(X):\n",
    "    # split data\n",
    "    X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "    y_train, y_test = y.iloc[train_ix], y.iloc[test_ix]\n",
    "\n",
    "    sfs1 = sfs_(model, \n",
    "               k_features=len(X.columns),\n",
    "               forward=True, \n",
    "               floating=False, \n",
    "               scoring='neg_mean_squared_error',\n",
    "               cv=cv)\n",
    "\n",
    "    pipe = Pipeline([\n",
    "                    #('standardize', ZCA()),\n",
    "                     ('sfs', sfs1)#, \n",
    "                     #('lr', model)\n",
    "                    ])\n",
    "\n",
    "    param_grid = {\n",
    "      }\n",
    "\n",
    "    CV_Linear_regressor = GridSearchCV (estimator = pipe,\n",
    "                                   param_grid = param_grid,\n",
    "                                   #cv = cv,return_train_score=True, verbose=0, refit=True)\n",
    "                                   verbose=0, refit=True)\n",
    "\n",
    "    CV_Linear_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    #ypred=CV_ENet_regressor.predict(X_test)\n",
    "\n",
    "    best_model = CV_Linear_regressor.best_estimator_\n",
    "    #plt.plot(pd.DataFrame(best_model.named_steps['sfs'].subsets_).loc['avg_score'])\n",
    "\n",
    "    bestSFS=X.columns[pd.Series(pd.DataFrame(best_model.named_steps['sfs'].subsets_).iloc[:,np.argmin(np.abs(pd.DataFrame(best_model.named_steps['sfs'].subsets_).loc['avg_score']))-1].feature_idx).to_list()]\n",
    "    print(bestSFS)\n",
    "    lm = LinearRegression()\n",
    "    lm.fit(X_train[bestSFS],y_train)\n",
    "    \n",
    "    #print(best_model.named_steps['lr'].coef_)\n",
    "    yhat = lm.predict(X_test[bestSFS])\n",
    "\n",
    "    # evaluate the model\n",
    "    acc = mean_squared_error(y_test, yhat,squared=True)\n",
    "    #cross_val_score(self.search, X, y, scoring='neg_mean_squared_error', cv=cv_outer, n_jobs=-1)\n",
    "    # store the result\n",
    "    outer_results.append(acc)\n",
    "    \n",
    "print('accuracy: %.3f (%.3f)' % (mean(outer_results), std(outer_results)))\n",
    "\n",
    "model_results.append(['Linear',mean(outer_results), std(outer_results)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b36bc60-3e3e-4ae9-84c0-414f860ad6be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343cfe38-4912-4124-b273-c7783c61738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in range(0,len(model_results)):\n",
    "    print('Model: ', model_results[s][0])\n",
    "    print('Accuracy: %.3f (%.3f)' % (model_results[s][1], model_results[s][2]))\n",
    "\n",
    "#step 1\n",
    "best_model = [item[0] for item in model_results][np.argmin(np.abs([item[1] for item in model_results]))]\n",
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536149b2-1c2d-4693-a1ed-727fed9989ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cbc8c4-4317-4ba2-994a-0de66048561e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if(best_model=='ElasticNetCV'):\n",
    "    zca = ZCA()\n",
    "    zca.fit(X)\n",
    "    \n",
    "    #Step 2: inner-procedure is applied to the entire dataset.\n",
    "    final_model_est = Pipeline(estimators_ENetCV)\n",
    "    final_model_est.fit(X,y)\n",
    "    #final_model_est = ElasticNetCV(alphas=[final_model_est.named_steps['ElasticNetCV'].alpha_], l1_ratio=final_model_est.named_steps['ElasticNetCV'].l1_ratio,fit_intercept=True)\n",
    "    \n",
    "    #final_model_est.fit(zca.transform(X), y)\n",
    "    \n",
    "    #step 3: hyperparameters found during this final search are then used to configure a final model.\n",
    "    estimators_final = []\n",
    "    #estimators_final.append(('standardize', ZCA()))\n",
    "    estimators_final.append(('ElasticNet', ElasticNetCV(cv=cv_inner,alphas=[final_model_est.named_steps['ElasticNetCV'].alpha_], l1_ratio=final_model_est.named_steps['ElasticNetCV'].l1_ratio,fit_intercept=True)))\n",
    "    final_model = Pipeline(estimators_final)\n",
    "    \n",
    "    final_model.fit(zca.transform(X),y)    \n",
    "    \n",
    "    #step 4: final model is fit on the entire dataset.\n",
    "    plt.scatter(final_model.predict(zca.transform(X)),y)\n",
    "    plt.show()\n",
    "    print(mean_squared_error(y, final_model.predict(zca.transform(X)), squared=True))\n",
    "    \n",
    "    #Step 2: inner-procedure is applied to the entire dataset.\n",
    "    final_model_est = Pipeline(estimators_ENetCV)\n",
    "    final_model_est.fit(X,y)\n",
    "    #final_model_est = ElasticNetCV(alphas=[final_model_est.named_steps['ElasticNetCV'].alpha_], l1_ratio=final_model_est.named_steps['ElasticNetCV'].l1_ratio,fit_intercept=True)\n",
    "    \n",
    "    #final_model_est.fit(zca.transform(X), y)\n",
    "    \n",
    "    #step 3: hyperparameters found during this final search are then used to configure a final model.\n",
    "    estimators_final = []\n",
    "    #estimators_final.append(('standardize', ZCA()))\n",
    "    estimators_final.append(('ElasticNet', ElasticNetCV(cv=cv_inner,alphas=[final_model_est.named_steps['ElasticNetCV'].alpha_], l1_ratio=final_model_est.named_steps['ElasticNetCV'].l1_ratio,fit_intercept=True)))\n",
    "    final_model = Pipeline(estimators_final)\n",
    "    \n",
    "    final_model.fit(final_model_est.named_steps['standardize'].transform(X),y)    \n",
    "    \n",
    "    #step 4: final model is fit on the entire dataset.\n",
    "    plt.scatter(final_model.predict(final_model_est.named_steps['standardize'].transform(X)),y)\n",
    "    plt.show()\n",
    "    print(mean_squared_error(y, final_model.predict(final_model_est.named_steps['standardize'].transform(X)), squared=True))\n",
    "        \n",
    "elif(best_model=='linear_enet'):\n",
    "    final_model_est = Pipeline(estimators_linear)\n",
    "    \n",
    "    final_model_est.fit(X,y)\n",
    "    \n",
    "    bestSFS=X.columns[pd.Series(pd.DataFrame(final_model_est.named_steps['SFS'].subsets_).iloc[:,np.argmin(np.abs(pd.DataFrame(final_model_est.named_steps['SFS'].subsets_).loc['avg_score']))-1].feature_idx).to_list()]\n",
    "    \n",
    "    #Step 2: inner-procedure is applied to the entire dataset.\n",
    "    #step 3: hyperparameters found during this final search are then used to configure a final model.\n",
    "    #no need to derive hyper parm's\n",
    "    final_model = ElasticNetCV(cv=cv_inner,alphas=[0], l1_ratio=0,fit_intercept=True)\n",
    "    \n",
    "    zca = ZCA()\n",
    "    zca.fit(X[bestSFS])\n",
    "    \n",
    "    final_model.fit(zca.transform(X[bestSFS]), y)\n",
    "    \n",
    "    #final_model.fit(zca.transform(X),y)    \n",
    "    \n",
    "    #step 4: final model is fit on the entire dataset.\n",
    "    plt.scatter(final_model.predict(zca.transform(X[bestSFS])),y)\n",
    "    plt.show()\n",
    "    print(mean_squared_error(y, final_model.predict(zca.transform(X[bestSFS])), squared=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419990a6-c118-4dfd-89b1-e6aaf7a31e3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
