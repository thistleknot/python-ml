{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a5667b-d3e5-4ea0-9657-6ebf9d030f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences =  [\"John works at Google in California.\",\n",
    "                 \"Mary lives in Paris and works for Microsoft.\",\n",
    "                 \"The Eiffel Tower is located in France.\",\n",
    "                 \"Paris is located in France.\",\n",
    "                 \"John experienced an earthquake.\",\n",
    "                 \"John is 35 years old.\",\n",
    "                 \"Josh hates pizza.\",\n",
    "                 \"When Mary is depressed, she likes to go shopping at the mall.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc914d-eee9-4855-b995-aae11b23547a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define input text\n",
    "text = \"Mary lives in Paris and works for Microsoft.\"\n",
    "#for text in sentences:\n",
    "    # Create a Doc object\n",
    "doc = nlp(text)\n",
    "\n",
    "# Extract subject, predicate, and object from the input text\n",
    "for token in doc:\n",
    "    if \"subj\" in token.dep_:\n",
    "        subject = token.text\n",
    "    if \"obj\" in token.dep_:\n",
    "        obj = token.text\n",
    "    if \"ROOT\" in token.dep_:\n",
    "        predicate = token.text\n",
    "\n",
    "# Print the extracted subject, predicate, and object\n",
    "print(\"Subject:\", subject)\n",
    "print(\"Predicate:\", predicate)\n",
    "print(\"Object:\", obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c77a05-a936-4fb1-9337-4a3dfa6bed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "import json\n",
    "\n",
    "\n",
    "# Define input text\n",
    "#text = \"John works at Google in California.\"\n",
    "for text in sentences:\n",
    "    # Create a client object for Stanford OpenIE\n",
    "    nlp = StanfordCoreNLP('http://192.168.3.100:9000')\n",
    "\n",
    "    # Annotate the input text with OpenIE triplets\n",
    "    output_str = nlp.annotate(text, properties={\n",
    "        'annotators': 'openie',\n",
    "        'outputFormat': 'json'\n",
    "    })\n",
    "    output = json.loads(output_str)\n",
    "\n",
    "    # Extract the OpenIE triplets from the output\n",
    "    for sentence in output['sentences']:\n",
    "        for triplet in sentence['openie']:\n",
    "            subj = triplet['subject']\n",
    "            rel = triplet['relation']\n",
    "            obj = triplet['object']\n",
    "            print(subj, rel,obj)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb61b59-2e05-4673-84af-d95e3a8be838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "146e8018-0934-4b81-9e72-0dfacb4c77bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " John    \n",
      " Mary    \n",
      "     \n",
      "     \n",
      " John   earthquake \n",
      " John    \n",
      " Josh   pizza \n",
      "When Mary depressed go shopping mall\n"
     ]
    }
   ],
   "source": [
    "from pycorenlp import StanfordCoreNLP\n",
    "import json\n",
    "\n",
    "# Define input text\n",
    "text = \"When Mary is depressed, she likes to go shopping at the mall.\"\n",
    "\n",
    "# Create a client object for Stanford CoreNLP\n",
    "nlp = StanfordCoreNLP('http://192.168.3.100:9000')\n",
    "\n",
    "for text in sentences:\n",
    "    # Annotate the input text with dependency parsing\n",
    "    output_str = nlp.annotate(text, properties={\n",
    "        'annotators': 'tokenize,ssplit,pos,depparse',\n",
    "        'outputFormat': 'json'\n",
    "    })\n",
    "    output = json.loads(output_str)\n",
    "\n",
    "    # Extract relevant information from the dependency parse\n",
    "    conditional = \"\"\n",
    "    subject = \"\"\n",
    "    adjective = \"\"\n",
    "    action = \"\"\n",
    "    obj = \"\"\n",
    "    nmod = \"\"\n",
    "\n",
    "    for sentence in output['sentences']:\n",
    "        for dep in sentence['enhancedPlusPlusDependencies']:\n",
    "            #print(dep)\n",
    "            if dep['dep'] == 'advmod':            \n",
    "                conditional = dep['dependentGloss']\n",
    "                #print('advmod', conditional)\n",
    "            elif dep['dep'] == 'nsubj':            \n",
    "                # Get the POS tag of the dependent token\n",
    "                pos_tag = sentence['tokens'][dep['dependent'] - 1]['pos']\n",
    "                # Check if the POS tag is not a pronoun\n",
    "                if pos_tag not in ['PRP', 'PRP$']:\n",
    "                    subject = dep['dependentGloss']\n",
    "                    #print('nsubj', dep['dependentGloss'])\n",
    "            elif dep['dep'] == 'advcl':\n",
    "                adjective = dep['dependentGloss']\n",
    "                #print('advcl',adjective)\n",
    "            elif dep['dep'] == 'amod':\n",
    "                #print('amod')\n",
    "                adjective = dep['dependentGloss']\n",
    "            elif dep['dep'] =='xcomp':\n",
    "                #print('xcomp')\n",
    "                action = dep['dependentGloss']\n",
    "            elif dep['dep'] == 'obj':\n",
    "                #print('dobj')\n",
    "                obj = dep['dependentGloss']\n",
    "            elif dep['dep'] == 'nmod:at':\n",
    "                #print('nmod:at')\n",
    "                nmod = dep['dependentGloss']            \n",
    "\n",
    "    print(conditional, subject, adjective, action, obj, nmod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "66ef4434-a413-4464-985e-b3372ed036f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 1,\n",
       "  'word': 'When',\n",
       "  'originalText': 'When',\n",
       "  'characterOffsetBegin': 0,\n",
       "  'characterOffsetEnd': 4,\n",
       "  'pos': 'WRB',\n",
       "  'before': '',\n",
       "  'after': ' '},\n",
       " {'index': 2,\n",
       "  'word': 'Mary',\n",
       "  'originalText': 'Mary',\n",
       "  'characterOffsetBegin': 5,\n",
       "  'characterOffsetEnd': 9,\n",
       "  'pos': 'NNP',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 3,\n",
       "  'word': 'is',\n",
       "  'originalText': 'is',\n",
       "  'characterOffsetBegin': 10,\n",
       "  'characterOffsetEnd': 12,\n",
       "  'pos': 'VBZ',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 4,\n",
       "  'word': 'depressed',\n",
       "  'originalText': 'depressed',\n",
       "  'characterOffsetBegin': 13,\n",
       "  'characterOffsetEnd': 22,\n",
       "  'pos': 'JJ',\n",
       "  'before': ' ',\n",
       "  'after': ''},\n",
       " {'index': 5,\n",
       "  'word': ',',\n",
       "  'originalText': ',',\n",
       "  'characterOffsetBegin': 22,\n",
       "  'characterOffsetEnd': 23,\n",
       "  'pos': ',',\n",
       "  'before': '',\n",
       "  'after': ' '},\n",
       " {'index': 6,\n",
       "  'word': 'she',\n",
       "  'originalText': 'she',\n",
       "  'characterOffsetBegin': 24,\n",
       "  'characterOffsetEnd': 27,\n",
       "  'pos': 'PRP',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 7,\n",
       "  'word': 'likes',\n",
       "  'originalText': 'likes',\n",
       "  'characterOffsetBegin': 28,\n",
       "  'characterOffsetEnd': 33,\n",
       "  'pos': 'VBZ',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 8,\n",
       "  'word': 'to',\n",
       "  'originalText': 'to',\n",
       "  'characterOffsetBegin': 34,\n",
       "  'characterOffsetEnd': 36,\n",
       "  'pos': 'TO',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 9,\n",
       "  'word': 'go',\n",
       "  'originalText': 'go',\n",
       "  'characterOffsetBegin': 37,\n",
       "  'characterOffsetEnd': 39,\n",
       "  'pos': 'VB',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 10,\n",
       "  'word': 'shopping',\n",
       "  'originalText': 'shopping',\n",
       "  'characterOffsetBegin': 40,\n",
       "  'characterOffsetEnd': 48,\n",
       "  'pos': 'NN',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 11,\n",
       "  'word': 'at',\n",
       "  'originalText': 'at',\n",
       "  'characterOffsetBegin': 49,\n",
       "  'characterOffsetEnd': 51,\n",
       "  'pos': 'IN',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 12,\n",
       "  'word': 'the',\n",
       "  'originalText': 'the',\n",
       "  'characterOffsetBegin': 52,\n",
       "  'characterOffsetEnd': 55,\n",
       "  'pos': 'DT',\n",
       "  'before': ' ',\n",
       "  'after': ' '},\n",
       " {'index': 13,\n",
       "  'word': 'mall',\n",
       "  'originalText': 'mall',\n",
       "  'characterOffsetBegin': 56,\n",
       "  'characterOffsetEnd': 60,\n",
       "  'pos': 'NN',\n",
       "  'before': ' ',\n",
       "  'after': ''},\n",
       " {'index': 14,\n",
       "  'word': '.',\n",
       "  'originalText': '.',\n",
       "  'characterOffsetBegin': 60,\n",
       "  'characterOffsetEnd': 61,\n",
       "  'pos': '.',\n",
       "  'before': '',\n",
       "  'after': ''}]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence['tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a466f8-b0ad-425a-a940-a70e9f1f227a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917063d3-190d-4ea7-aa4a-52c1133a4374",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0e5267-2a6a-40b4-87bf-26147cb08167",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55489f9-ece9-404a-9043-f2dbf22d9dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = ' '.join(word['word'] for word in triplet['subject'] if word['pos'] not in pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fb09bf-d672-4f03-968e-ff48e64ec101",
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b5571d-30a3-4718-af37-9fe5a52b7fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "[word['word'] for word in triplet['subject'] if word['pos'] not in pos_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84314da5-7630-417b-b98c-59ea79bcf62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import spacy\n",
    "import textacy\n",
    "\n",
    "# Load English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define input text\n",
    "for text in sentences:\n",
    "    # Create a Doc object\n",
    "\n",
    "    # Create a Doc object\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract subject-verb-object triplets from the input text\n",
    "    triplets = textacy.extract.subject_verb_object_triples(doc)\n",
    "\n",
    "    # Print the extracted triplets\n",
    "    for triplet in triplets:\n",
    "        print(triplet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e70d6c-ae8d-42bb-8ea2-7fd24716788c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from openie import StanfordOpenIE\n",
    "\n",
    "# Define input texttext = \"John works at Google in California.\"\n",
    "\n",
    "# Create a client object for Stanford OpenIE\n",
    "with StanfordOpenIE() as client:\n",
    "    # Extract all triplets from the input text using Stanford OpenIE\n",
    "    for text in sentences:\n",
    "        for triplet in client.annotate(text):\n",
    "            print(triplet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134fe3c8-0a8d-4cc5-a6e5-d7c124b925ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from allennlp.predictors.predictor import Predictor\n",
    "\n",
    "# Define input text\n",
    "for text in sentences:\n",
    "#text = \"John works at Google in California.\"\n",
    "\n",
    "    # Create a predictor object for AllenNLP OpenIE model\n",
    "    predictor = Predictor.from_path(\"https://storage.googleapis.com/allennlp-public-models/openie-model.2020.03.26.tar.gz\")\n",
    "\n",
    "    # Extract all triplets from the input text using AllenNLP OpenIE model\n",
    "    results = predictor.predict(sentence=text)\n",
    "    for triplet in results[\"verbs\"]:\n",
    "        print(triplet[\"description\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
